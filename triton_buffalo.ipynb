{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxKeaa04c8Nx"
      },
      "outputs": [],
      "source": [
        "!pip install onnx onnxruntime-gpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "euSkDS5ZL5Sm"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y onnx onnxruntime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "_wp9u8SlJAAG",
        "outputId": "7b2f4edd-314b-4e49-975f-978bbec84fa8"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'onnx'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-165313683.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monnxruntime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnx'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# @Organization  : insightface.ai\n",
        "# @Author        : Jia Guo\n",
        "# @Time          : 2021-09-18\n",
        "# @Function      :\n",
        "\n",
        "from __future__ import division\n",
        "import datetime\n",
        "import numpy as np\n",
        "import onnx\n",
        "import onnxruntime\n",
        "import os\n",
        "import os.path as osp\n",
        "import cv2\n",
        "import sys\n",
        "\n",
        "def softmax(z):\n",
        "    assert len(z.shape) == 2\n",
        "    s = np.max(z, axis=1)\n",
        "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
        "    e_x = np.exp(z - s)\n",
        "    div = np.sum(e_x, axis=1)\n",
        "    div = div[:, np.newaxis] # dito\n",
        "    return e_x / div\n",
        "\n",
        "def distance2bbox(points, distance, max_shape=None):\n",
        "    \"\"\"Decode distance prediction to bounding box.\n",
        "\n",
        "    Args:\n",
        "        points (Tensor): Shape (n, 2), [x, y].\n",
        "        distance (Tensor): Distance from the given point to 4\n",
        "            boundaries (left, top, right, bottom).\n",
        "        max_shape (tuple): Shape of the image.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Decoded bboxes.\n",
        "    \"\"\"\n",
        "    x1 = points[:, 0] - distance[:, 0]\n",
        "    y1 = points[:, 1] - distance[:, 1]\n",
        "    x2 = points[:, 0] + distance[:, 2]\n",
        "    y2 = points[:, 1] + distance[:, 3]\n",
        "    if max_shape is not None:\n",
        "        x1 = x1.clamp(min=0, max=max_shape[1])\n",
        "        y1 = y1.clamp(min=0, max=max_shape[0])\n",
        "        x2 = x2.clamp(min=0, max=max_shape[1])\n",
        "        y2 = y2.clamp(min=0, max=max_shape[0])\n",
        "    return np.stack([x1, y1, x2, y2], axis=-1)\n",
        "\n",
        "def distance2kps(points, distance, max_shape=None):\n",
        "    \"\"\"Decode distance prediction to bounding box.\n",
        "\n",
        "    Args:\n",
        "        points (Tensor): Shape (n, 2), [x, y].\n",
        "        distance (Tensor): Distance from the given point to 4\n",
        "            boundaries (left, top, right, bottom).\n",
        "        max_shape (tuple): Shape of the image.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Decoded bboxes.\n",
        "    \"\"\"\n",
        "    preds = []\n",
        "    for i in range(0, distance.shape[1], 2):\n",
        "        px = points[:, i%2] + distance[:, i]\n",
        "        py = points[:, i%2+1] + distance[:, i+1]\n",
        "        if max_shape is not None:\n",
        "            px = px.clamp(min=0, max=max_shape[1])\n",
        "            py = py.clamp(min=0, max=max_shape[0])\n",
        "        preds.append(px)\n",
        "        preds.append(py)\n",
        "    return np.stack(preds, axis=-1)\n",
        "\n",
        "class RetinaFace:\n",
        "    def __init__(self, model_file=None, session=None):\n",
        "        import onnxruntime\n",
        "        self.model_file = model_file\n",
        "        self.session = session\n",
        "        self.taskname = 'detection'\n",
        "        if self.session is None:\n",
        "            assert self.model_file is not None\n",
        "            assert osp.exists(self.model_file)\n",
        "            self.session = onnxruntime.InferenceSession(self.model_file, None)\n",
        "        self.center_cache = {}\n",
        "        self.nms_thresh = 0.4\n",
        "        self.det_thresh = 0.5\n",
        "        self._init_vars()\n",
        "\n",
        "    def _init_vars(self):\n",
        "        input_cfg = self.session.get_inputs()[0]\n",
        "        input_shape = input_cfg.shape\n",
        "        #print(input_shape)\n",
        "        if isinstance(input_shape[2], str):\n",
        "            self.input_size = None\n",
        "        else:\n",
        "            self.input_size = tuple(input_shape[2:4][::-1])\n",
        "        #print('image_size:', self.image_size)\n",
        "        input_name = input_cfg.name\n",
        "        self.input_shape = input_shape\n",
        "        outputs = self.session.get_outputs()\n",
        "        output_names = []\n",
        "        for o in outputs:\n",
        "            output_names.append(o.name)\n",
        "        self.input_name = input_name\n",
        "        self.output_names = output_names\n",
        "        self.input_mean = 127.5\n",
        "        self.input_std = 128.0\n",
        "        #print(self.output_names)\n",
        "        #assert len(outputs)==10 or len(outputs)==15\n",
        "        self.use_kps = False\n",
        "        self._anchor_ratio = 1.0\n",
        "        self._num_anchors = 1\n",
        "        if len(outputs)==6:\n",
        "            self.fmc = 3\n",
        "            self._feat_stride_fpn = [8, 16, 32]\n",
        "            self._num_anchors = 2\n",
        "        elif len(outputs)==9:\n",
        "            self.fmc = 3\n",
        "            self._feat_stride_fpn = [8, 16, 32]\n",
        "            self._num_anchors = 2\n",
        "            self.use_kps = True\n",
        "        elif len(outputs)==10:\n",
        "            self.fmc = 5\n",
        "            self._feat_stride_fpn = [8, 16, 32, 64, 128]\n",
        "            self._num_anchors = 1\n",
        "        elif len(outputs)==15:\n",
        "            self.fmc = 5\n",
        "            self._feat_stride_fpn = [8, 16, 32, 64, 128]\n",
        "            self._num_anchors = 1\n",
        "            self.use_kps = True\n",
        "\n",
        "    def prepare(self, ctx_id, **kwargs):\n",
        "        if ctx_id<0:\n",
        "            self.session.set_providers(['CPUExecutionProvider'])\n",
        "        nms_thresh = kwargs.get('nms_thresh', None)\n",
        "        if nms_thresh is not None:\n",
        "            self.nms_thresh = nms_thresh\n",
        "        det_thresh = kwargs.get('det_thresh', None)\n",
        "        if det_thresh is not None:\n",
        "            self.det_thresh = det_thresh\n",
        "        input_size = kwargs.get('input_size', None)\n",
        "        if input_size is not None:\n",
        "            if self.input_size is not None:\n",
        "                print('warning: det_size is already set in detection model, ignore')\n",
        "            else:\n",
        "                self.input_size = input_size\n",
        "\n",
        "    def forward(self, img, threshold):\n",
        "        scores_list = []\n",
        "        bboxes_list = []\n",
        "        kpss_list = []\n",
        "        input_size = tuple(img.shape[0:2][::-1])\n",
        "        blob = cv2.dnn.blobFromImage(img, 1.0/self.input_std, input_size, (self.input_mean, self.input_mean, self.input_mean), swapRB=True)\n",
        "        net_outs = self.session.run(self.output_names, {self.input_name : blob})\n",
        "\n",
        "        input_height = blob.shape[2]\n",
        "        input_width = blob.shape[3]\n",
        "        fmc = self.fmc\n",
        "        for idx, stride in enumerate(self._feat_stride_fpn):\n",
        "            scores = net_outs[idx]\n",
        "            bbox_preds = net_outs[idx+fmc]\n",
        "            bbox_preds = bbox_preds * stride\n",
        "            if self.use_kps:\n",
        "                kps_preds = net_outs[idx+fmc*2] * stride\n",
        "            height = input_height // stride\n",
        "            width = input_width // stride\n",
        "            K = height * width\n",
        "            key = (height, width, stride)\n",
        "            if key in self.center_cache:\n",
        "                anchor_centers = self.center_cache[key]\n",
        "            else:\n",
        "                #solution-1, c style:\n",
        "                #anchor_centers = np.zeros( (height, width, 2), dtype=np.float32 )\n",
        "                #for i in range(height):\n",
        "                #    anchor_centers[i, :, 1] = i\n",
        "                #for i in range(width):\n",
        "                #    anchor_centers[:, i, 0] = i\n",
        "\n",
        "                #solution-2:\n",
        "                #ax = np.arange(width, dtype=np.float32)\n",
        "                #ay = np.arange(height, dtype=np.float32)\n",
        "                #xv, yv = np.meshgrid(np.arange(width), np.arange(height))\n",
        "                #anchor_centers = np.stack([xv, yv], axis=-1).astype(np.float32)\n",
        "\n",
        "                #solution-3:\n",
        "                anchor_centers = np.stack(np.mgrid[:height, :width][::-1], axis=-1).astype(np.float32)\n",
        "                #print(anchor_centers.shape)\n",
        "\n",
        "                anchor_centers = (anchor_centers * stride).reshape( (-1, 2) )\n",
        "                if self._num_anchors>1:\n",
        "                    anchor_centers = np.stack([anchor_centers]*self._num_anchors, axis=1).reshape( (-1,2) )\n",
        "                if len(self.center_cache)<100:\n",
        "                    self.center_cache[key] = anchor_centers\n",
        "\n",
        "            pos_inds = np.where(scores>=threshold)[0]\n",
        "            bboxes = distance2bbox(anchor_centers, bbox_preds)\n",
        "            pos_scores = scores[pos_inds]\n",
        "            pos_bboxes = bboxes[pos_inds]\n",
        "            scores_list.append(pos_scores)\n",
        "            bboxes_list.append(pos_bboxes)\n",
        "            if self.use_kps:\n",
        "                kpss = distance2kps(anchor_centers, kps_preds)\n",
        "                #kpss = kps_preds\n",
        "                kpss = kpss.reshape( (kpss.shape[0], -1, 2) )\n",
        "                pos_kpss = kpss[pos_inds]\n",
        "                kpss_list.append(pos_kpss)\n",
        "        return scores_list, bboxes_list, kpss_list\n",
        "\n",
        "    def detect(self, img, input_size = None, max_num=0, metric='default'):\n",
        "        assert input_size is not None or self.input_size is not None\n",
        "        input_size = self.input_size if input_size is None else input_size\n",
        "\n",
        "        im_ratio = float(img.shape[0]) / img.shape[1]\n",
        "        model_ratio = float(input_size[1]) / input_size[0]\n",
        "        if im_ratio>model_ratio:\n",
        "            new_height = input_size[1]\n",
        "            new_width = int(new_height / im_ratio)\n",
        "        else:\n",
        "            new_width = input_size[0]\n",
        "            new_height = int(new_width * im_ratio)\n",
        "        det_scale = float(new_height) / img.shape[0]\n",
        "        resized_img = cv2.resize(img, (new_width, new_height))\n",
        "        det_img = np.zeros( (input_size[1], input_size[0], 3), dtype=np.uint8 )\n",
        "        det_img[:new_height, :new_width, :] = resized_img\n",
        "\n",
        "        scores_list, bboxes_list, kpss_list = self.forward(det_img, self.det_thresh)\n",
        "\n",
        "        scores = np.vstack(scores_list)\n",
        "        scores_ravel = scores.ravel()\n",
        "        order = scores_ravel.argsort()[::-1]\n",
        "        bboxes = np.vstack(bboxes_list) / det_scale\n",
        "        if self.use_kps:\n",
        "            kpss = np.vstack(kpss_list) / det_scale\n",
        "        pre_det = np.hstack((bboxes, scores)).astype(np.float32, copy=False)\n",
        "        pre_det = pre_det[order, :]\n",
        "        keep = self.nms(pre_det)\n",
        "        det = pre_det[keep, :]\n",
        "        if self.use_kps:\n",
        "            kpss = kpss[order,:,:]\n",
        "            kpss = kpss[keep,:,:]\n",
        "        else:\n",
        "            kpss = None\n",
        "        if max_num > 0 and det.shape[0] > max_num:\n",
        "            area = (det[:, 2] - det[:, 0]) * (det[:, 3] -\n",
        "                                                    det[:, 1])\n",
        "            img_center = img.shape[0] // 2, img.shape[1] // 2\n",
        "            offsets = np.vstack([\n",
        "                (det[:, 0] + det[:, 2]) / 2 - img_center[1],\n",
        "                (det[:, 1] + det[:, 3]) / 2 - img_center[0]\n",
        "            ])\n",
        "            offset_dist_squared = np.sum(np.power(offsets, 2.0), 0)\n",
        "            if metric=='max':\n",
        "                values = area\n",
        "            else:\n",
        "                values = area - offset_dist_squared * 2.0  # some extra weight on the centering\n",
        "            bindex = np.argsort(\n",
        "                values)[::-1]  # some extra weight on the centering\n",
        "            bindex = bindex[0:max_num]\n",
        "            det = det[bindex, :]\n",
        "            if kpss is not None:\n",
        "                kpss = kpss[bindex, :]\n",
        "        return det, kpss\n",
        "\n",
        "    def nms(self, dets):\n",
        "        thresh = self.nms_thresh\n",
        "        x1 = dets[:, 0]\n",
        "        y1 = dets[:, 1]\n",
        "        x2 = dets[:, 2]\n",
        "        y2 = dets[:, 3]\n",
        "        scores = dets[:, 4]\n",
        "\n",
        "        areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "        order = scores.argsort()[::-1]\n",
        "\n",
        "        keep = []\n",
        "        while order.size > 0:\n",
        "            i = order[0]\n",
        "            keep.append(i)\n",
        "            xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "            yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "            xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "            yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "\n",
        "            w = np.maximum(0.0, xx2 - xx1 + 1)\n",
        "            h = np.maximum(0.0, yy2 - yy1 + 1)\n",
        "            inter = w * h\n",
        "            ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
        "\n",
        "            inds = np.where(ovr <= thresh)[0]\n",
        "            order = order[inds + 1]\n",
        "\n",
        "        return keep\n",
        "\n",
        "def get_retinaface(name, download=False, root='~/.insightface/models', **kwargs):\n",
        "    if not download:\n",
        "        assert os.path.exists(name)\n",
        "        return RetinaFace(name)\n",
        "    else:\n",
        "        from .model_store import get_model_file\n",
        "        _file = get_model_file(\"retinaface_%s\" % name, root=root)\n",
        "        return retinaface(_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McdQ-hJYJBEH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm as l2norm\n",
        "#from easydict import EasyDict\n",
        "\n",
        "class Face(dict):\n",
        "\n",
        "    def __init__(self, d=None, **kwargs):\n",
        "        if d is None:\n",
        "            d = {}\n",
        "        if kwargs:\n",
        "            d.update(**kwargs)\n",
        "        for k, v in d.items():\n",
        "            setattr(self, k, v)\n",
        "        # Class attributes\n",
        "        #for k in self.__class__.__dict__.keys():\n",
        "        #    if not (k.startswith('__') and k.endswith('__')) and not k in ('update', 'pop'):\n",
        "        #        setattr(self, k, getattr(self, k))\n",
        "\n",
        "    def __setattr__(self, name, value):\n",
        "        if isinstance(value, (list, tuple)):\n",
        "            value = [self.__class__(x)\n",
        "                    if isinstance(x, dict) else x for x in value]\n",
        "        elif isinstance(value, dict) and not isinstance(value, self.__class__):\n",
        "            value = self.__class__(value)\n",
        "        super(Face, self).__setattr__(name, value)\n",
        "        super(Face, self).__setitem__(name, value)\n",
        "\n",
        "    __setitem__ = __setattr__\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        return None\n",
        "\n",
        "    @property\n",
        "    def embedding_norm(self):\n",
        "        if self.embedding is None:\n",
        "            return None\n",
        "        return l2norm(self.embedding)\n",
        "\n",
        "    @property\n",
        "    def normed_embedding(self):\n",
        "        if self.embedding is None:\n",
        "            return None\n",
        "        return self.embedding / self.embedding_norm\n",
        "\n",
        "    @property\n",
        "    def sex(self):\n",
        "        if self.gender is None:\n",
        "            return None\n",
        "        return 'M' if self.gender==1 else 'F'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5R6pjlPJS6m",
        "outputId": "9782781f-bf27-4e79-bcf9-4a9ceed19a93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[250.83516   105.717285  458.21808   421.5759      0.9008425]]\n",
            "[[[306.65668 233.22589]\n",
            "  [407.13126 231.2535 ]\n",
            "  [357.61658 293.05417]\n",
            "  [314.43353 343.88757]\n",
            "  [396.18857 342.39694]]]\n",
            "{'bbox': array([250.83516 , 105.717285, 458.21808 , 421.5759  ], dtype=float32), 'kps': array([[306.65668, 233.22589],\n",
            "       [407.13126, 231.2535 ],\n",
            "       [357.61658, 293.05417],\n",
            "       [314.43353, 343.88757],\n",
            "       [396.18857, 342.39694]], dtype=float32), 'det_score': np.float32(0.9008425)}\n",
            "Bounding boxes:\n",
            " [[250.83516   105.717285  458.21808   421.5759      0.9008425]]\n",
            "Keypoints:\n",
            " [[[306.65668 233.22589]\n",
            "  [407.13126 231.2535 ]\n",
            "  [357.61658 293.05417]\n",
            "  [314.43353 343.88757]\n",
            "  [396.18857 342.39694]]]\n",
            "Saved result as retinaface_result.jpg\n"
          ]
        }
      ],
      "source": [
        "# ---- RetinaFace test ----\n",
        "import cv2\n",
        "\n",
        "# Load the model\n",
        "detector = RetinaFace(model_file=\"/content/model.onnx\")\n",
        "detector.prepare(ctx_id=0, input_size=(640, 640))  # GPU (0) or CPU (-1)\n",
        "\n",
        "# Load an image\n",
        "# img = cv2.imread(\"Mahesh - Supervisor(1).jpg\")\n",
        "# img=cv2.imread(\"Rakesh - Worker(1).jpg\")\n",
        "img=cv2.imread(\"/content/face.jpg\")\n",
        "\n",
        "# Run detection\n",
        "bboxes, kpss = detector.detect(img, input_size=(640, 640))\n",
        "print(bboxes)\n",
        "print(kpss)\n",
        "\n",
        "for i in range(bboxes.shape[0]):\n",
        "    bbox = bboxes[i, 0:4]\n",
        "    det_score = bboxes[i, 4]\n",
        "    kps = None\n",
        "    if kpss is not None:\n",
        "        kps = kpss[i]\n",
        "    face = Face(bbox=bbox, kps=kps, det_score=det_score)\n",
        "print(face)\n",
        "print(\"Bounding boxes:\\n\", bboxes)\n",
        "if kpss is not None:\n",
        "    print(\"Keypoints:\\n\", kpss)\n",
        "\n",
        "# Optionally visualize\n",
        "for box in bboxes:\n",
        "    x1, y1, x2, y2, score = box.astype(int)\n",
        "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "    cv2.putText(img, f\"{score:.2f}\", (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)\n",
        "cv2.imwrite(\"retinaface_result.jpg\", img)\n",
        "print(\"Saved result as retinaface_result.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I2gv6d7IP-B"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import transform as trans\n",
        "\n",
        "\n",
        "arcface_dst = np.array(\n",
        "    [[38.2946, 51.6963], [73.5318, 51.5014], [56.0252, 71.7366],\n",
        "     [41.5493, 92.3655], [70.7299, 92.2041]],\n",
        "    dtype=np.float32)\n",
        "\n",
        "def estimate_norm(lmk, image_size=112,mode='arcface'):\n",
        "    assert lmk.shape == (5, 2)\n",
        "    assert image_size%112==0 or image_size%128==0\n",
        "    if image_size%112==0:\n",
        "        ratio = float(image_size)/112.0\n",
        "        diff_x = 0\n",
        "    else:\n",
        "        ratio = float(image_size)/128.0\n",
        "        diff_x = 8.0*ratio\n",
        "    dst = arcface_dst * ratio\n",
        "    dst[:,0] += diff_x\n",
        "    tform = trans.SimilarityTransform()\n",
        "    tform.estimate(lmk, dst)\n",
        "    M = tform.params[0:2, :]\n",
        "    return M\n",
        "\n",
        "def norm_crop(img, landmark, image_size=112, mode='arcface'):\n",
        "    M = estimate_norm(landmark, image_size, mode)\n",
        "    warped = cv2.warpAffine(img, M, (image_size, image_size), borderValue=0.0)\n",
        "    return warped\n",
        "\n",
        "def norm_crop2(img, landmark, image_size=112, mode='arcface'):\n",
        "    M = estimate_norm(landmark, image_size, mode)\n",
        "    warped = cv2.warpAffine(img, M, (image_size, image_size), borderValue=0.0)\n",
        "    return warped, M\n",
        "\n",
        "def square_crop(im, S):\n",
        "    if im.shape[0] > im.shape[1]:\n",
        "        height = S\n",
        "        width = int(float(im.shape[1]) / im.shape[0] * S)\n",
        "        scale = float(S) / im.shape[0]\n",
        "    else:\n",
        "        width = S\n",
        "        height = int(float(im.shape[0]) / im.shape[1] * S)\n",
        "        scale = float(S) / im.shape[1]\n",
        "    resized_im = cv2.resize(im, (width, height))\n",
        "    det_im = np.zeros((S, S, 3), dtype=np.uint8)\n",
        "    det_im[:resized_im.shape[0], :resized_im.shape[1], :] = resized_im\n",
        "    return det_im, scale\n",
        "\n",
        "\n",
        "def transform(data, center, output_size, scale, rotation):\n",
        "    scale_ratio = scale\n",
        "    rot = float(rotation) * np.pi / 180.0\n",
        "    #translation = (output_size/2-center[0]*scale_ratio, output_size/2-center[1]*scale_ratio)\n",
        "    t1 = trans.SimilarityTransform(scale=scale_ratio)\n",
        "    cx = center[0] * scale_ratio\n",
        "    cy = center[1] * scale_ratio\n",
        "    t2 = trans.SimilarityTransform(translation=(-1 * cx, -1 * cy))\n",
        "    t3 = trans.SimilarityTransform(rotation=rot)\n",
        "    t4 = trans.SimilarityTransform(translation=(output_size / 2,\n",
        "                                                output_size / 2))\n",
        "    t = t1 + t2 + t3 + t4\n",
        "    M = t.params[0:2]\n",
        "    cropped = cv2.warpAffine(data,\n",
        "                             M, (output_size, output_size),\n",
        "                             borderValue=0.0)\n",
        "    return cropped, M\n",
        "\n",
        "\n",
        "def trans_points2d(pts, M):\n",
        "    new_pts = np.zeros(shape=pts.shape, dtype=np.float32)\n",
        "    for i in range(pts.shape[0]):\n",
        "        pt = pts[i]\n",
        "        new_pt = np.array([pt[0], pt[1], 1.], dtype=np.float32)\n",
        "        new_pt = np.dot(M, new_pt)\n",
        "        #print('new_pt', new_pt.shape, new_pt)\n",
        "        new_pts[i] = new_pt[0:2]\n",
        "\n",
        "    return new_pts\n",
        "\n",
        "\n",
        "def trans_points3d(pts, M):\n",
        "    scale = np.sqrt(M[0][0] * M[0][0] + M[0][1] * M[0][1])\n",
        "    #print(scale)\n",
        "    new_pts = np.zeros(shape=pts.shape, dtype=np.float32)\n",
        "    for i in range(pts.shape[0]):\n",
        "        pt = pts[i]\n",
        "        new_pt = np.array([pt[0], pt[1], 1.], dtype=np.float32)\n",
        "        new_pt = np.dot(M, new_pt)\n",
        "        #print('new_pt', new_pt.shape, new_pt)\n",
        "        new_pts[i][0:2] = new_pt[0:2]\n",
        "        new_pts[i][2] = pts[i][2] * scale\n",
        "\n",
        "    return new_pts\n",
        "\n",
        "\n",
        "def trans_points(pts, M):\n",
        "    if pts.shape[1] == 2:\n",
        "        return trans_points2d(pts, M)\n",
        "    else:\n",
        "        return trans_points3d(pts, M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCPmBHBMIPtR"
      },
      "outputs": [],
      "source": [
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "import cv2\n",
        "import onnx\n",
        "import onnxruntime\n",
        "# from ..utils import face_align\n",
        "\n",
        "__all__ = [\n",
        "    'ArcFaceONNX',\n",
        "]\n",
        "\n",
        "\n",
        "class ArcFaceONNX:\n",
        "    def __init__(self, model_file=None, session=None):\n",
        "        assert model_file is not None\n",
        "        self.model_file = model_file\n",
        "        self.session = session\n",
        "        self.taskname = 'recognition'\n",
        "        find_sub = False\n",
        "        find_mul = False\n",
        "        model = onnx.load(self.model_file)\n",
        "        graph = model.graph\n",
        "        for nid, node in enumerate(graph.node[:8]):\n",
        "            #print(nid, node.name)\n",
        "            if node.name.startswith('Sub') or node.name.startswith('_minus'):\n",
        "                find_sub = True\n",
        "            if node.name.startswith('Mul') or node.name.startswith('_mul'):\n",
        "                find_mul = True\n",
        "        if find_sub and find_mul:\n",
        "            #mxnet arcface model\n",
        "            input_mean = 0.0\n",
        "            input_std = 1.0\n",
        "        else:\n",
        "            input_mean = 127.5\n",
        "            input_std = 127.5\n",
        "        self.input_mean = input_mean\n",
        "        self.input_std = input_std\n",
        "        #print('input mean and std:', self.input_mean, self.input_std)\n",
        "        if self.session is None:\n",
        "            self.session = onnxruntime.InferenceSession(self.model_file, None)\n",
        "        input_cfg = self.session.get_inputs()[0]\n",
        "        input_shape = input_cfg.shape\n",
        "        input_name = input_cfg.name\n",
        "        self.input_size = tuple(input_shape[2:4][::-1])\n",
        "        self.input_shape = input_shape\n",
        "        outputs = self.session.get_outputs()\n",
        "        output_names = []\n",
        "        for out in outputs:\n",
        "            output_names.append(out.name)\n",
        "        self.input_name = input_name\n",
        "        self.output_names = output_names\n",
        "        assert len(self.output_names)==1\n",
        "        self.output_shape = outputs[0].shape\n",
        "\n",
        "    def prepare(self, ctx_id, **kwargs):\n",
        "        if ctx_id<0:\n",
        "            self.session.set_providers(['CPUExecutionProvider'])\n",
        "\n",
        "    def get(self, img, face):\n",
        "        aimg = norm_crop(img, landmark=face.kps, image_size=self.input_size[0])\n",
        "        face.embedding = self.get_feat(aimg).flatten()\n",
        "        return face.embedding\n",
        "\n",
        "    def compute_sim(self, feat1, feat2):\n",
        "        from numpy.linalg import norm\n",
        "        feat1 = feat1.ravel()\n",
        "        feat2 = feat2.ravel()\n",
        "        sim = np.dot(feat1, feat2) / (norm(feat1) * norm(feat2))\n",
        "        return sim\n",
        "\n",
        "    def get_feat(self, imgs):\n",
        "        if not isinstance(imgs, list):\n",
        "            imgs = [imgs]\n",
        "        input_size = self.input_size\n",
        "\n",
        "        blob = cv2.dnn.blobFromImages(imgs, 1.0 / self.input_std, input_size,\n",
        "                                      (self.input_mean, self.input_mean, self.input_mean), swapRB=True)\n",
        "        net_out = self.session.run(self.output_names, {self.input_name: blob})[0]\n",
        "        return net_out\n",
        "\n",
        "    def forward(self, batch_data):\n",
        "        blob = (batch_data - self.input_mean) / self.input_std\n",
        "        net_out = self.session.run(self.output_names, {self.input_name: blob})[0]\n",
        "        return net_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2ST-p3pIPRn"
      },
      "outputs": [],
      "source": [
        "# ---- RetinaFace test ----\n",
        "import cv2\n",
        "\n",
        "# Load the model\n",
        "embedder = ArcFaceONNX(model_file=\"/content/w600k_r50.onnx\")\n",
        "embedder.prepare(ctx_id=0, input_size=(640, 640))  # GPU (0) or CPU (-1)\n",
        "\n",
        "# Load an image\n",
        "# img = cv2.imread(\"Mahesh - Supervisor(1).jpg\")\n",
        "img=cv2.imread(\"face.jpg\")\n",
        "\n",
        "embedder.get(img,face)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iw_8JBkJT9b"
      },
      "outputs": [],
      "source": [
        "with cpu triton on azure vm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_KHb7fAb_PfX"
      },
      "outputs": [],
      "source": [
        "!pip install nvidia-pyindex\n",
        "!pip install tritonclient[http]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQWITxbrOTbH"
      },
      "outputs": [],
      "source": [
        "import tritonclient.http as httpclient\n",
        "from tritonclient.utils import np_to_triton_dtype\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os.path as osp\n",
        "\n",
        "def softmax(z):\n",
        "    assert len(z.shape) == 2\n",
        "    s = np.max(z, axis=1)\n",
        "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
        "    e_x = np.exp(z - s)\n",
        "    div = np.sum(e_x, axis=1)\n",
        "    div = div[:, np.newaxis] # dito\n",
        "    return e_x / div\n",
        "\n",
        "def distance2bbox(points, distance, max_shape=None):\n",
        "    \"\"\"Decode distance prediction to bounding box.\n",
        "\n",
        "    Args:\n",
        "        points (Tensor): Shape (n, 2), [x, y].\n",
        "        distance (Tensor): Distance from the given point to 4\n",
        "            boundaries (left, top, right, bottom).\n",
        "        max_shape (tuple): Shape of the image.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Decoded bboxes.\n",
        "    \"\"\"\n",
        "    x1 = points[:, 0] - distance[:, 0]\n",
        "    y1 = points[:, 1] - distance[:, 1]\n",
        "    x2 = points[:, 0] + distance[:, 2]\n",
        "    y2 = points[:, 1] + distance[:, 3]\n",
        "    if max_shape is not None:\n",
        "        x1 = x1.clamp(min=0, max=max_shape[1])\n",
        "        y1 = y1.clamp(min=0, max=max_shape[0])\n",
        "        x2 = x2.clamp(min=0, max=max_shape[1])\n",
        "        y2 = y2.clamp(min=0, max=max_shape[0])\n",
        "    return np.stack([x1, y1, x2, y2], axis=-1)\n",
        "\n",
        "def distance2kps(points, distance, max_shape=None):\n",
        "    \"\"\"Decode distance prediction to bounding box.\n",
        "\n",
        "    Args:\n",
        "        points (Tensor): Shape (n, 2), [x, y].\n",
        "        distance (Tensor): Distance from the given point to 4\n",
        "            boundaries (left, top, right, bottom).\n",
        "        max_shape (tuple): Shape of the image.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Decoded bboxes.\n",
        "    \"\"\"\n",
        "    preds = []\n",
        "    for i in range(0, distance.shape[1], 2):\n",
        "        px = points[:, i%2] + distance[:, i]\n",
        "        py = points[:, i%2+1] + distance[:, i+1]\n",
        "        if max_shape is not None:\n",
        "            px = px.clamp(min=0, max=max_shape[1])\n",
        "            py = py.clamp(min=0, max=max_shape[0])\n",
        "        preds.append(px)\n",
        "        preds.append(py)\n",
        "    return np.stack(preds, axis=-1)\n",
        "\n",
        "class RetinaFace:\n",
        "    def __init__(self, model_file=None, session=None, triton_url=\"4.150.202.198:9000\", model_name=\"buffalo_detection\"):\n",
        "        # remove onnxruntime, use Triton client instead\n",
        "        self.model_file = model_file\n",
        "        self.taskname = 'detection'\n",
        "\n",
        "        # Triton client setup\n",
        "        self.triton_client = httpclient.InferenceServerClient(url=triton_url)\n",
        "        self.model_name = model_name\n",
        "\n",
        "        self.center_cache = {}\n",
        "        self.nms_thresh = 0.4\n",
        "        self.det_thresh = 0.5\n",
        "\n",
        "        # since no onnx session, directly configure input/output names from config\n",
        "        self._init_vars()\n",
        "\n",
        "    def _init_vars(self):\n",
        "        # Manually set from your config file\n",
        "        self.input_name = \"input.1\"\n",
        "        self.output_names = [\n",
        "            \"448\",\"471\",\"494\",  # scores\n",
        "            \"451\",\"474\",\"497\",  # bboxes\n",
        "            \"454\",\"477\",\"500\"   # kps\n",
        "        ]\n",
        "        self.input_mean = 127.5\n",
        "        self.input_std = 128.0\n",
        "        self.use_kps = True  # because you have 9 outputs with keypoints\n",
        "        self.fmc = 3\n",
        "        self._feat_stride_fpn = [8, 16, 32]\n",
        "        self._num_anchors = 2\n",
        "        self.input_size = None  # dynamic\n",
        "\n",
        "    def prepare(self, ctx_id, **kwargs):\n",
        "        # Triton handles CPU/GPU internally, so we ignore ctx_id\n",
        "        nms_thresh = kwargs.get('nms_thresh', None)\n",
        "        if nms_thresh is not None:\n",
        "            self.nms_thresh = nms_thresh\n",
        "        det_thresh = kwargs.get('det_thresh', None)\n",
        "        if det_thresh is not None:\n",
        "            self.det_thresh = det_thresh\n",
        "        input_size = kwargs.get('input_size', None)\n",
        "        if input_size is not None:\n",
        "            if self.input_size is not None:\n",
        "                print('warning: det_size is already set in detection model, ignore')\n",
        "            else:\n",
        "                self.input_size = input_size\n",
        "\n",
        "    def forward(self, img, threshold):\n",
        "        scores_list = []\n",
        "        bboxes_list = []\n",
        "        kpss_list = []\n",
        "        input_size = tuple(img.shape[0:2][::-1])\n",
        "\n",
        "        blob = cv2.dnn.blobFromImage(img, 1.0/self.input_std, input_size,\n",
        "                                     (self.input_mean, self.input_mean, self.input_mean),\n",
        "                                     swapRB=True)\n",
        "\n",
        "        # Triton input\n",
        "        inputs = [httpclient.InferInput(self.input_name, blob.shape, np_to_triton_dtype(blob.dtype))]\n",
        "        inputs[0].set_data_from_numpy(blob)\n",
        "\n",
        "        # Triton outputs\n",
        "        outputs = [httpclient.InferRequestedOutput(name) for name in self.output_names]\n",
        "\n",
        "        # run inference\n",
        "        results = self.triton_client.infer(model_name=self.model_name, inputs=inputs, outputs=outputs)\n",
        "\n",
        "        net_outs = [results.as_numpy(name) for name in self.output_names]\n",
        "\n",
        "        input_height = blob.shape[2]\n",
        "        input_width = blob.shape[3]\n",
        "        fmc = self.fmc\n",
        "        for idx, stride in enumerate(self._feat_stride_fpn):\n",
        "            scores = net_outs[idx]\n",
        "            bbox_preds = net_outs[idx+fmc] * stride\n",
        "            if self.use_kps:\n",
        "                kps_preds = net_outs[idx+fmc*2] * stride\n",
        "\n",
        "            height = input_height // stride\n",
        "            width = input_width // stride\n",
        "            K = height * width\n",
        "            key = (height, width, stride)\n",
        "            if key in self.center_cache:\n",
        "                anchor_centers = self.center_cache[key]\n",
        "            else:\n",
        "                anchor_centers = np.stack(np.mgrid[:height, :width][::-1], axis=-1).astype(np.float32)\n",
        "                anchor_centers = (anchor_centers * stride).reshape((-1, 2))\n",
        "                if self._num_anchors>1:\n",
        "                    anchor_centers = np.stack([anchor_centers]*self._num_anchors, axis=1).reshape((-1,2))\n",
        "                if len(self.center_cache)<100:\n",
        "                    self.center_cache[key] = anchor_centers\n",
        "\n",
        "            pos_inds = np.where(scores>=threshold)[0]\n",
        "            bboxes = distance2bbox(anchor_centers, bbox_preds)\n",
        "            pos_scores = scores[pos_inds]\n",
        "            pos_bboxes = bboxes[pos_inds]\n",
        "            scores_list.append(pos_scores)\n",
        "            bboxes_list.append(pos_bboxes)\n",
        "            if self.use_kps:\n",
        "                kpss = distance2kps(anchor_centers, kps_preds)\n",
        "                kpss = kpss.reshape((kpss.shape[0], -1, 2))\n",
        "                pos_kpss = kpss[pos_inds]\n",
        "                kpss_list.append(pos_kpss)\n",
        "        return scores_list, bboxes_list, kpss_list\n",
        "\n",
        "\n",
        "    def detect(self, img, input_size = None, max_num=0, metric='default'):\n",
        "        assert input_size is not None or self.input_size is not None\n",
        "        input_size = self.input_size if input_size is None else input_size\n",
        "\n",
        "        im_ratio = float(img.shape[0]) / img.shape[1]\n",
        "        model_ratio = float(input_size[1]) / input_size[0]\n",
        "        if im_ratio>model_ratio:\n",
        "            new_height = input_size[1]\n",
        "            new_width = int(new_height / im_ratio)\n",
        "        else:\n",
        "            new_width = input_size[0]\n",
        "            new_height = int(new_width * im_ratio)\n",
        "        det_scale = float(new_height) / img.shape[0]\n",
        "        resized_img = cv2.resize(img, (new_width, new_height))\n",
        "        det_img = np.zeros( (input_size[1], input_size[0], 3), dtype=np.uint8 )\n",
        "        det_img[:new_height, :new_width, :] = resized_img\n",
        "\n",
        "        scores_list, bboxes_list, kpss_list = self.forward(det_img, self.det_thresh)\n",
        "\n",
        "        scores = np.vstack(scores_list)\n",
        "        scores_ravel = scores.ravel()\n",
        "        order = scores_ravel.argsort()[::-1]\n",
        "        bboxes = np.vstack(bboxes_list) / det_scale\n",
        "        if self.use_kps:\n",
        "            kpss = np.vstack(kpss_list) / det_scale\n",
        "        pre_det = np.hstack((bboxes, scores)).astype(np.float32, copy=False)\n",
        "        pre_det = pre_det[order, :]\n",
        "        keep = self.nms(pre_det)\n",
        "        det = pre_det[keep, :]\n",
        "        if self.use_kps:\n",
        "            kpss = kpss[order,:,:]\n",
        "            kpss = kpss[keep,:,:]\n",
        "        else:\n",
        "            kpss = None\n",
        "        if max_num > 0 and det.shape[0] > max_num:\n",
        "            area = (det[:, 2] - det[:, 0]) * (det[:, 3] -\n",
        "                                                    det[:, 1])\n",
        "            img_center = img.shape[0] // 2, img.shape[1] // 2\n",
        "            offsets = np.vstack([\n",
        "                (det[:, 0] + det[:, 2]) / 2 - img_center[1],\n",
        "                (det[:, 1] + det[:, 3]) / 2 - img_center[0]\n",
        "            ])\n",
        "            offset_dist_squared = np.sum(np.power(offsets, 2.0), 0)\n",
        "            if metric=='max':\n",
        "                values = area\n",
        "            else:\n",
        "                values = area - offset_dist_squared * 2.0  # some extra weight on the centering\n",
        "            bindex = np.argsort(\n",
        "                values)[::-1]  # some extra weight on the centering\n",
        "            bindex = bindex[0:max_num]\n",
        "            det = det[bindex, :]\n",
        "            if kpss is not None:\n",
        "                kpss = kpss[bindex, :]\n",
        "        return det, kpss\n",
        "\n",
        "    def nms(self, dets):\n",
        "        thresh = self.nms_thresh\n",
        "        x1 = dets[:, 0]\n",
        "        y1 = dets[:, 1]\n",
        "        x2 = dets[:, 2]\n",
        "        y2 = dets[:, 3]\n",
        "        scores = dets[:, 4]\n",
        "\n",
        "        areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "        order = scores.argsort()[::-1]\n",
        "\n",
        "        keep = []\n",
        "        while order.size > 0:\n",
        "            i = order[0]\n",
        "            keep.append(i)\n",
        "            xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "            yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "            xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "            yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "\n",
        "            w = np.maximum(0.0, xx2 - xx1 + 1)\n",
        "            h = np.maximum(0.0, yy2 - yy1 + 1)\n",
        "            inter = w * h\n",
        "            ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
        "\n",
        "            inds = np.where(ovr <= thresh)[0]\n",
        "            order = order[inds + 1]\n",
        "\n",
        "        return keep\n",
        "\n",
        "def get_retinaface(name, download=False, root='~/.insightface/models', **kwargs):\n",
        "    if not download:\n",
        "        assert os.path.exists(name)\n",
        "        return RetinaFace(name)\n",
        "    else:\n",
        "        from .model_store import get_model_file\n",
        "        _file = get_model_file(\"retinaface_%s\" % name, root=root)\n",
        "        return retinaface(_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRVKVkHuRIBI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm as l2norm\n",
        "#from easydict import EasyDict\n",
        "\n",
        "class Face(dict):\n",
        "\n",
        "    def __init__(self, d=None, **kwargs):\n",
        "        if d is None:\n",
        "            d = {}\n",
        "        if kwargs:\n",
        "            d.update(**kwargs)\n",
        "        for k, v in d.items():\n",
        "            setattr(self, k, v)\n",
        "        # Class attributes\n",
        "        #for k in self.__class__.__dict__.keys():\n",
        "        #    if not (k.startswith('__') and k.endswith('__')) and not k in ('update', 'pop'):\n",
        "        #        setattr(self, k, getattr(self, k))\n",
        "\n",
        "    def __setattr__(self, name, value):\n",
        "        if isinstance(value, (list, tuple)):\n",
        "            value = [self.__class__(x)\n",
        "                    if isinstance(x, dict) else x for x in value]\n",
        "        elif isinstance(value, dict) and not isinstance(value, self.__class__):\n",
        "            value = self.__class__(value)\n",
        "        super(Face, self).__setattr__(name, value)\n",
        "        super(Face, self).__setitem__(name, value)\n",
        "\n",
        "    __setitem__ = __setattr__\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        return None\n",
        "\n",
        "    @property\n",
        "    def embedding_norm(self):\n",
        "        if self.embedding is None:\n",
        "            return None\n",
        "        return l2norm(self.embedding)\n",
        "\n",
        "    @property\n",
        "    def normed_embedding(self):\n",
        "        if self.embedding is None:\n",
        "            return None\n",
        "        return self.embedding / self.embedding_norm\n",
        "\n",
        "    @property\n",
        "    def sex(self):\n",
        "        if self.gender is None:\n",
        "            return None\n",
        "        return 'M' if self.gender==1 else 'F'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UeR81enRP0x",
        "outputId": "53a597ce-6386-4ad3-f7df-f8a56366f2fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bounding boxes:\n",
            " [[250.83516   105.71726   458.21802   421.5759      0.9008425]]\n",
            "Keypoints:\n",
            " [[[306.65668 233.22589]\n",
            "  [407.13126 231.2535 ]\n",
            "  [357.61658 293.05417]\n",
            "  [314.43347 343.88757]\n",
            "  [396.18857 342.39694]]]\n",
            "{'bbox': array([250.83516, 105.71726, 458.21802, 421.5759 ], dtype=float32), 'kps': array([[306.65668, 233.22589],\n",
            "       [407.13126, 231.2535 ],\n",
            "       [357.61658, 293.05417],\n",
            "       [314.43347, 343.88757],\n",
            "       [396.18857, 342.39694]], dtype=float32), 'det_score': np.float32(0.9008425)}\n",
            "Saved result as retinaface_result.jpg\n"
          ]
        }
      ],
      "source": [
        "# ---- RetinaFace test (Triton version) ----\n",
        "import cv2\n",
        "\n",
        "# Load the model using Triton backend\n",
        "# point to your Triton server URL + model name\n",
        "detector = RetinaFace(\n",
        "    triton_url=\"4.150.202.198:9000\",   # e.g. http://4.150.202.198:8000\n",
        "    model_name=\"buffalo_detection\"\n",
        ")\n",
        "\n",
        "# Prepare input size (same as before)\n",
        "detector.prepare(ctx_id=0, input_size=(640, 640))  # Triton CPU/GPU handled internally\n",
        "\n",
        "# Load an image\n",
        "img = cv2.imread(\"face.jpg\")\n",
        "\n",
        "\n",
        "\n",
        "# Run detection\n",
        "bboxes, kpss = detector.detect(img, input_size=(640, 640))\n",
        "print(\"Bounding boxes:\\n\", bboxes)\n",
        "print(\"Keypoints:\\n\", kpss)\n",
        "\n",
        "# Build Face objects (same as before)\n",
        "for i in range(bboxes.shape[0]):\n",
        "    bbox = bboxes[i, 0:4]\n",
        "    det_score = bboxes[i, 4]\n",
        "    kps = None\n",
        "    if kpss is not None:\n",
        "        kps = kpss[i]\n",
        "    face = Face(bbox=bbox, kps=kps, det_score=det_score)\n",
        "    print(face)\n",
        "\n",
        "# Optionally visualize\n",
        "for box in bboxes:\n",
        "    x1, y1, x2, y2, score = box.astype(int)\n",
        "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "    cv2.putText(img, f\"{score:.2f}\", (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)\n",
        "\n",
        "cv2.imwrite(\"retinaface_result.jpg\", img)\n",
        "print(\"Saved result as retinaface_result.jpg\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9sRbyX98mcP"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import transform as trans\n",
        "\n",
        "\n",
        "arcface_dst = np.array(\n",
        "    [[38.2946, 51.6963], [73.5318, 51.5014], [56.0252, 71.7366],\n",
        "     [41.5493, 92.3655], [70.7299, 92.2041]],\n",
        "    dtype=np.float32)\n",
        "\n",
        "def estimate_norm(lmk, image_size=112,mode='arcface'):\n",
        "    assert lmk.shape == (5, 2)\n",
        "    assert image_size%112==0 or image_size%128==0\n",
        "    if image_size%112==0:\n",
        "        ratio = float(image_size)/112.0\n",
        "        diff_x = 0\n",
        "    else:\n",
        "        ratio = float(image_size)/128.0\n",
        "        diff_x = 8.0*ratio\n",
        "    dst = arcface_dst * ratio\n",
        "    dst[:,0] += diff_x\n",
        "    tform = trans.SimilarityTransform()\n",
        "    tform.estimate(lmk, dst)\n",
        "    M = tform.params[0:2, :]\n",
        "    return M\n",
        "\n",
        "def norm_crop(img, landmark, image_size=112, mode='arcface'):\n",
        "    M = estimate_norm(landmark, image_size, mode)\n",
        "    warped = cv2.warpAffine(img, M, (image_size, image_size), borderValue=0.0)\n",
        "    return warped\n",
        "\n",
        "def norm_crop2(img, landmark, image_size=112, mode='arcface'):\n",
        "    M = estimate_norm(landmark, image_size, mode)\n",
        "    warped = cv2.warpAffine(img, M, (image_size, image_size), borderValue=0.0)\n",
        "    return warped, M\n",
        "\n",
        "def square_crop(im, S):\n",
        "    if im.shape[0] > im.shape[1]:\n",
        "        height = S\n",
        "        width = int(float(im.shape[1]) / im.shape[0] * S)\n",
        "        scale = float(S) / im.shape[0]\n",
        "    else:\n",
        "        width = S\n",
        "        height = int(float(im.shape[0]) / im.shape[1] * S)\n",
        "        scale = float(S) / im.shape[1]\n",
        "    resized_im = cv2.resize(im, (width, height))\n",
        "    det_im = np.zeros((S, S, 3), dtype=np.uint8)\n",
        "    det_im[:resized_im.shape[0], :resized_im.shape[1], :] = resized_im\n",
        "    return det_im, scale\n",
        "\n",
        "\n",
        "def transform(data, center, output_size, scale, rotation):\n",
        "    scale_ratio = scale\n",
        "    rot = float(rotation) * np.pi / 180.0\n",
        "    #translation = (output_size/2-center[0]*scale_ratio, output_size/2-center[1]*scale_ratio)\n",
        "    t1 = trans.SimilarityTransform(scale=scale_ratio)\n",
        "    cx = center[0] * scale_ratio\n",
        "    cy = center[1] * scale_ratio\n",
        "    t2 = trans.SimilarityTransform(translation=(-1 * cx, -1 * cy))\n",
        "    t3 = trans.SimilarityTransform(rotation=rot)\n",
        "    t4 = trans.SimilarityTransform(translation=(output_size / 2,\n",
        "                                                output_size / 2))\n",
        "    t = t1 + t2 + t3 + t4\n",
        "    M = t.params[0:2]\n",
        "    cropped = cv2.warpAffine(data,\n",
        "                             M, (output_size, output_size),\n",
        "                             borderValue=0.0)\n",
        "    return cropped, M\n",
        "\n",
        "\n",
        "def trans_points2d(pts, M):\n",
        "    new_pts = np.zeros(shape=pts.shape, dtype=np.float32)\n",
        "    for i in range(pts.shape[0]):\n",
        "        pt = pts[i]\n",
        "        new_pt = np.array([pt[0], pt[1], 1.], dtype=np.float32)\n",
        "        new_pt = np.dot(M, new_pt)\n",
        "        #print('new_pt', new_pt.shape, new_pt)\n",
        "        new_pts[i] = new_pt[0:2]\n",
        "\n",
        "    return new_pts\n",
        "\n",
        "\n",
        "def trans_points3d(pts, M):\n",
        "    scale = np.sqrt(M[0][0] * M[0][0] + M[0][1] * M[0][1])\n",
        "    #print(scale)\n",
        "    new_pts = np.zeros(shape=pts.shape, dtype=np.float32)\n",
        "    for i in range(pts.shape[0]):\n",
        "        pt = pts[i]\n",
        "        new_pt = np.array([pt[0], pt[1], 1.], dtype=np.float32)\n",
        "        new_pt = np.dot(M, new_pt)\n",
        "        #print('new_pt', new_pt.shape, new_pt)\n",
        "        new_pts[i][0:2] = new_pt[0:2]\n",
        "        new_pts[i][2] = pts[i][2] * scale\n",
        "\n",
        "    return new_pts\n",
        "\n",
        "\n",
        "def trans_points(pts, M):\n",
        "    if pts.shape[1] == 2:\n",
        "        return trans_points2d(pts, M)\n",
        "    else:\n",
        "        return trans_points3d(pts, M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJgErtFZAcM1"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tritonclient.http as httpclient  # Triton client\n",
        "\n",
        "__all__ = [\n",
        "    'ArcFaceONNX',\n",
        "]\n",
        "\n",
        "class ArcFaceONNX:\n",
        "    def __init__(self, model_name=None, url=\"4.150.202.198:9000\"):\n",
        "        assert model_name is not None, \"Provide Triton model name\"\n",
        "        self.model_name = model_name\n",
        "        self.taskname = 'recognition'\n",
        "        self.url = url\n",
        "\n",
        "        # Triton client\n",
        "        self.client = httpclient.InferenceServerClient(url=self.url)\n",
        "\n",
        "        # ArcFace default preprocessing (mxnet vs normal)\n",
        "        # Adjust as needed for your model\n",
        "        self.input_mean = 127.5\n",
        "        self.input_std = 127.5\n",
        "\n",
        "        # Get model metadata from Triton\n",
        "        model_metadata = self.client.get_model_metadata(model_name=self.model_name)\n",
        "        model_config = self.client.get_model_config(model_name=self.model_name)\n",
        "\n",
        "        # Extract input name, output name, and input shape\n",
        "        self.input_name = model_metadata['inputs'][0]['name']\n",
        "        self.output_names = [o['name'] for o in model_metadata['outputs']]\n",
        "        self.input_shape = model_metadata['inputs'][0]['shape']\n",
        "        self.input_size = (self.input_shape[2], self.input_shape[3])  # (W, H)\n",
        "        self.output_shape = model_metadata['outputs'][0]['shape']\n",
        "\n",
        "    def prepare(self, ctx_id, **kwargs):\n",
        "        pass  # Triton handles CPU/GPU automatically on the server\n",
        "\n",
        "    def get(self, img, face):\n",
        "        aimg = norm_crop(img, landmark=face.kps, image_size=self.input_size[0])\n",
        "        face.embedding = self.get_feat(aimg).flatten()\n",
        "        return face.embedding\n",
        "\n",
        "    def compute_sim(self, feat1, feat2):\n",
        "        from numpy.linalg import norm\n",
        "        feat1 = feat1.ravel()\n",
        "        feat2 = feat2.ravel()\n",
        "        sim = np.dot(feat1, feat2) / (norm(feat1) * norm(feat2))\n",
        "        return sim\n",
        "\n",
        "    def get_feat(self, imgs):\n",
        "        if not isinstance(imgs, list):\n",
        "            imgs = [imgs]\n",
        "        input_size = self.input_size\n",
        "\n",
        "        blob = cv2.dnn.blobFromImages(\n",
        "            imgs,\n",
        "            1.0 / self.input_std,\n",
        "            input_size,\n",
        "            (self.input_mean, self.input_mean, self.input_mean),\n",
        "            swapRB=True\n",
        "        )\n",
        "\n",
        "        # Send to Triton\n",
        "        inputs = []\n",
        "        inputs.append(httpclient.InferInput(self.input_name, blob.shape, \"FP32\"))\n",
        "        inputs[0].set_data_from_numpy(blob)\n",
        "\n",
        "        outputs = []\n",
        "        for out_name in self.output_names:\n",
        "            outputs.append(httpclient.InferRequestedOutput(out_name))\n",
        "\n",
        "        result = self.client.infer(model_name=self.model_name, inputs=inputs, outputs=outputs)\n",
        "\n",
        "        net_out = result.as_numpy(self.output_names[0])\n",
        "        return net_out\n",
        "\n",
        "    def forward(self, batch_data):\n",
        "        blob = (batch_data - self.input_mean) / self.input_std\n",
        "\n",
        "        inputs = [httpclient.InferInput(self.input_name, blob.shape, \"FP32\")]\n",
        "        inputs[0].set_data_from_numpy(blob)\n",
        "\n",
        "        outputs = [httpclient.InferRequestedOutput(self.output_names[0])]\n",
        "        result = self.client.infer(model_name=self.model_name, inputs=inputs, outputs=outputs)\n",
        "\n",
        "        net_out = result.as_numpy(self.output_names[0])\n",
        "        return net_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reaMst_Xwjlu"
      },
      "outputs": [],
      "source": [
        "# ---- RetinaFace test ----\n",
        "import cv2\n",
        "\n",
        "# Load the model\n",
        "embedder = ArcFaceONNX(\n",
        "     model_name=\"buffalo_embedding\",\n",
        "     url=\"4.150.202.198:9000\"\n",
        ")\n",
        "embedder.prepare(ctx_id=0, input_size=(640, 640))  # GPU (0) or CPU (-1)\n",
        "\n",
        "# Load an image\n",
        "# img = cv2.imread(\"Mahesh - Supervisor(1).jpg\")\n",
        "img=cv2.imread(\"/content/face.jpg\")\n",
        "\n",
        "embedder.get(img,face)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E0eIwlhnKae1",
        "outputId": "13014658-02e5-4a87-dc56-31917b3a65ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tritonclient[all]\n",
            "  Downloading tritonclient-2.60.0-py3-none-manylinux1_x86_64.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.12/dist-packages (from tritonclient[all]) (2.0.2)\n",
            "Collecting python-rapidjson>=0.9.1 (from tritonclient[all])\n",
            "  Downloading python_rapidjson-1.21-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: urllib3>=2.0.7 in /usr/local/lib/python3.12/dist-packages (from tritonclient[all]) (2.5.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.1 in /usr/local/lib/python3.12/dist-packages (from tritonclient[all]) (3.12.15)\n",
            "Requirement already satisfied: cuda-python in /usr/local/lib/python3.12/dist-packages (from tritonclient[all]) (12.6.2.post1)\n",
            "Collecting geventhttpclient>=2.3.3 (from tritonclient[all])\n",
            "  Downloading geventhttpclient-2.3.4-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting grpcio<1.68,>=1.63.0 (from tritonclient[all])\n",
            "  Downloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.12/dist-packages (from tritonclient[all]) (25.0)\n",
            "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.12/dist-packages (from tritonclient[all]) (5.29.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (1.20.1)\n",
            "Collecting gevent (from geventhttpclient>=2.3.3->tritonclient[all])\n",
            "  Downloading gevent-25.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from geventhttpclient>=2.3.3->tritonclient[all]) (2025.8.3)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.12/dist-packages (from geventhttpclient>=2.3.3->tritonclient[all]) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (3.10)\n",
            "Requirement already satisfied: greenlet>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from gevent->geventhttpclient>=2.3.3->tritonclient[all]) (3.2.4)\n",
            "Collecting zope.event (from gevent->geventhttpclient>=2.3.3->tritonclient[all])\n",
            "  Downloading zope_event-6.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting zope.interface (from gevent->geventhttpclient>=2.3.3->tritonclient[all])\n",
            "  Downloading zope_interface-8.0.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setuptools>=75.8.2 (from zope.event->gevent->geventhttpclient>=2.3.3->tritonclient[all])\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Downloading geventhttpclient-2.3.4-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m115.0/115.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_rapidjson-1.21-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tritonclient-2.60.0-py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gevent-25.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope_event-6.0-py3-none-any.whl (6.4 kB)\n",
            "Downloading zope_interface-8.0.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (264 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m264.7/264.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zope.interface, setuptools, python-rapidjson, grpcio, zope.event, tritonclient, gevent, geventhttpclient\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.75.0\n",
            "    Uninstalling grpcio-1.75.0:\n",
            "      Successfully uninstalled grpcio-1.75.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "grpcio-status 1.71.2 requires grpcio>=1.71.2, but you have grpcio 1.67.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gevent-25.9.1 geventhttpclient-2.3.4 grpcio-1.67.1 python-rapidjson-1.21 setuptools-80.9.0 tritonclient-2.60.0 zope.event-6.0 zope.interface-8.0.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "654b5c6dd34e48e9ae4af396ec83013f",
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install tritonclient[all]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zC-Lu15aKdKv"
      },
      "outputs": [],
      "source": [
        "import tritonclient.http as httpclient\n",
        "from tritonclient.utils import np_to_triton_dtype\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os.path as osp\n",
        "\n",
        "def softmax(z):\n",
        "    assert len(z.shape) == 2\n",
        "    s = np.max(z, axis=1)\n",
        "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
        "    e_x = np.exp(z - s)\n",
        "    div = np.sum(e_x, axis=1)\n",
        "    div = div[:, np.newaxis] # dito\n",
        "    return e_x / div\n",
        "\n",
        "def distance2bbox(points, distance, max_shape=None):\n",
        "    \"\"\"Decode distance prediction to bounding box.\n",
        "\n",
        "    Args:\n",
        "        points (Tensor): Shape (n, 2), [x, y].\n",
        "        distance (Tensor): Distance from the given point to 4\n",
        "            boundaries (left, top, right, bottom).\n",
        "        max_shape (tuple): Shape of the image.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Decoded bboxes.\n",
        "    \"\"\"\n",
        "    x1 = points[:, 0] - distance[:, 0]\n",
        "    y1 = points[:, 1] - distance[:, 1]\n",
        "    x2 = points[:, 0] + distance[:, 2]\n",
        "    y2 = points[:, 1] + distance[:, 3]\n",
        "    if max_shape is not None:\n",
        "        x1 = x1.clamp(min=0, max=max_shape[1])\n",
        "        y1 = y1.clamp(min=0, max=max_shape[0])\n",
        "        x2 = x2.clamp(min=0, max=max_shape[1])\n",
        "        y2 = y2.clamp(min=0, max=max_shape[0])\n",
        "    return np.stack([x1, y1, x2, y2], axis=-1)\n",
        "\n",
        "def distance2kps(points, distance, max_shape=None):\n",
        "    \"\"\"Decode distance prediction to bounding box.\n",
        "\n",
        "    Args:\n",
        "        points (Tensor): Shape (n, 2), [x, y].\n",
        "        distance (Tensor): Distance from the given point to 4\n",
        "            boundaries (left, top, right, bottom).\n",
        "        max_shape (tuple): Shape of the image.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Decoded bboxes.\n",
        "    \"\"\"\n",
        "    preds = []\n",
        "    for i in range(0, distance.shape[1], 2):\n",
        "        px = points[:, i%2] + distance[:, i]\n",
        "        py = points[:, i%2+1] + distance[:, i+1]\n",
        "        if max_shape is not None:\n",
        "            px = px.clamp(min=0, max=max_shape[1])\n",
        "            py = py.clamp(min=0, max=max_shape[0])\n",
        "        preds.append(px)\n",
        "        preds.append(py)\n",
        "    return np.stack(preds, axis=-1)\n",
        "\n",
        "class RetinaFace:\n",
        "    def __init__(self, model_file=None, session=None, triton_url=\"psstr3clf1dpd1p1e075ccpuqs.ingress.rtx4090.wyo.eg.akash.pub\", model_name=\"buffalo_face_detection\"):\n",
        "        # remove onnxruntime, use Triton client instead\n",
        "        self.model_file = model_file\n",
        "        self.taskname = 'detection'\n",
        "\n",
        "        # Triton client setup\n",
        "        self.triton_client = httpclient.InferenceServerClient(url=triton_url,ssl=True, ssl_options={'verify': False})\n",
        "        self.model_name = model_name\n",
        "\n",
        "        self.center_cache = {}\n",
        "        self.nms_thresh = 0.4\n",
        "        self.det_thresh = 0.5\n",
        "\n",
        "        # since no onnx session, directly configure input/output names from config\n",
        "        self._init_vars()\n",
        "\n",
        "    def _init_vars(self):\n",
        "        # Manually set from your config file\n",
        "        self.input_name = \"input.1\"\n",
        "        self.output_names = [\n",
        "            \"448\",\"471\",\"494\",  # scores\n",
        "            \"451\",\"474\",\"497\",  # bboxes\n",
        "            \"454\",\"477\",\"500\"   # kps\n",
        "        ]\n",
        "        self.input_mean = 127.5\n",
        "        self.input_std = 128.0\n",
        "        self.use_kps = True  # because you have 9 outputs with keypoints\n",
        "        self.fmc = 3\n",
        "        self._feat_stride_fpn = [8, 16, 32]\n",
        "        self._num_anchors = 2\n",
        "        self.input_size = None  # dynamic\n",
        "\n",
        "    def prepare(self, ctx_id, **kwargs):\n",
        "        # Triton handles CPU/GPU internally, so we ignore ctx_id\n",
        "        nms_thresh = kwargs.get('nms_thresh', None)\n",
        "        if nms_thresh is not None:\n",
        "            self.nms_thresh = nms_thresh\n",
        "        det_thresh = kwargs.get('det_thresh', None)\n",
        "        if det_thresh is not None:\n",
        "            self.det_thresh = det_thresh\n",
        "        input_size = kwargs.get('input_size', None)\n",
        "        if input_size is not None:\n",
        "            if self.input_size is not None:\n",
        "                print('warning: det_size is already set in detection model, ignore')\n",
        "            else:\n",
        "                self.input_size = input_size\n",
        "\n",
        "    def forward(self, img, threshold):\n",
        "        scores_list = []\n",
        "        bboxes_list = []\n",
        "        kpss_list = []\n",
        "        input_size = tuple(img.shape[0:2][::-1])\n",
        "\n",
        "        blob = cv2.dnn.blobFromImage(img, 1.0/self.input_std, input_size,\n",
        "                                     (self.input_mean, self.input_mean, self.input_mean),\n",
        "                                     swapRB=True)\n",
        "\n",
        "        # Triton input\n",
        "        inputs = [httpclient.InferInput(self.input_name, blob.shape, np_to_triton_dtype(blob.dtype))]\n",
        "        inputs[0].set_data_from_numpy(blob)\n",
        "\n",
        "        # Triton outputs\n",
        "        outputs = [httpclient.InferRequestedOutput(name) for name in self.output_names]\n",
        "\n",
        "        # run inference\n",
        "        results = self.triton_client.infer(model_name=self.model_name, inputs=inputs, outputs=outputs)\n",
        "\n",
        "        net_outs = [results.as_numpy(name) for name in self.output_names]\n",
        "\n",
        "        input_height = blob.shape[2]\n",
        "        input_width = blob.shape[3]\n",
        "        fmc = self.fmc\n",
        "        for idx, stride in enumerate(self._feat_stride_fpn):\n",
        "            scores = net_outs[idx]\n",
        "            bbox_preds = net_outs[idx+fmc] * stride\n",
        "            if self.use_kps:\n",
        "                kps_preds = net_outs[idx+fmc*2] * stride\n",
        "\n",
        "            height = input_height // stride\n",
        "            width = input_width // stride\n",
        "            K = height * width\n",
        "            key = (height, width, stride)\n",
        "            if key in self.center_cache:\n",
        "                anchor_centers = self.center_cache[key]\n",
        "            else:\n",
        "                anchor_centers = np.stack(np.mgrid[:height, :width][::-1], axis=-1).astype(np.float32)\n",
        "                anchor_centers = (anchor_centers * stride).reshape((-1, 2))\n",
        "                if self._num_anchors>1:\n",
        "                    anchor_centers = np.stack([anchor_centers]*self._num_anchors, axis=1).reshape((-1,2))\n",
        "                if len(self.center_cache)<100:\n",
        "                    self.center_cache[key] = anchor_centers\n",
        "\n",
        "            pos_inds = np.where(scores>=threshold)[0]\n",
        "            bboxes = distance2bbox(anchor_centers, bbox_preds)\n",
        "            pos_scores = scores[pos_inds]\n",
        "            pos_bboxes = bboxes[pos_inds]\n",
        "            scores_list.append(pos_scores)\n",
        "            bboxes_list.append(pos_bboxes)\n",
        "            if self.use_kps:\n",
        "                kpss = distance2kps(anchor_centers, kps_preds)\n",
        "                kpss = kpss.reshape((kpss.shape[0], -1, 2))\n",
        "                pos_kpss = kpss[pos_inds]\n",
        "                kpss_list.append(pos_kpss)\n",
        "        return scores_list, bboxes_list, kpss_list\n",
        "\n",
        "\n",
        "    def detect(self, img, input_size = None, max_num=0, metric='default'):\n",
        "        assert input_size is not None or self.input_size is not None\n",
        "        input_size = self.input_size if input_size is None else input_size\n",
        "\n",
        "        im_ratio = float(img.shape[0]) / img.shape[1]\n",
        "        model_ratio = float(input_size[1]) / input_size[0]\n",
        "        if im_ratio>model_ratio:\n",
        "            new_height = input_size[1]\n",
        "            new_width = int(new_height / im_ratio)\n",
        "        else:\n",
        "            new_width = input_size[0]\n",
        "            new_height = int(new_width * im_ratio)\n",
        "        det_scale = float(new_height) / img.shape[0]\n",
        "        resized_img = cv2.resize(img, (new_width, new_height))\n",
        "        det_img = np.zeros( (input_size[1], input_size[0], 3), dtype=np.uint8 )\n",
        "        det_img[:new_height, :new_width, :] = resized_img\n",
        "\n",
        "        scores_list, bboxes_list, kpss_list = self.forward(det_img, self.det_thresh)\n",
        "\n",
        "        scores = np.vstack(scores_list)\n",
        "        scores_ravel = scores.ravel()\n",
        "        order = scores_ravel.argsort()[::-1]\n",
        "        bboxes = np.vstack(bboxes_list) / det_scale\n",
        "        if self.use_kps:\n",
        "            kpss = np.vstack(kpss_list) / det_scale\n",
        "        pre_det = np.hstack((bboxes, scores)).astype(np.float32, copy=False)\n",
        "        pre_det = pre_det[order, :]\n",
        "        keep = self.nms(pre_det)\n",
        "        det = pre_det[keep, :]\n",
        "        if self.use_kps:\n",
        "            kpss = kpss[order,:,:]\n",
        "            kpss = kpss[keep,:,:]\n",
        "        else:\n",
        "            kpss = None\n",
        "        if max_num > 0 and det.shape[0] > max_num:\n",
        "            area = (det[:, 2] - det[:, 0]) * (det[:, 3] -\n",
        "                                                    det[:, 1])\n",
        "            img_center = img.shape[0] // 2, img.shape[1] // 2\n",
        "            offsets = np.vstack([\n",
        "                (det[:, 0] + det[:, 2]) / 2 - img_center[1],\n",
        "                (det[:, 1] + det[:, 3]) / 2 - img_center[0]\n",
        "            ])\n",
        "            offset_dist_squared = np.sum(np.power(offsets, 2.0), 0)\n",
        "            if metric=='max':\n",
        "                values = area\n",
        "            else:\n",
        "                values = area - offset_dist_squared * 2.0  # some extra weight on the centering\n",
        "            bindex = np.argsort(\n",
        "                values)[::-1]  # some extra weight on the centering\n",
        "            bindex = bindex[0:max_num]\n",
        "            det = det[bindex, :]\n",
        "            if kpss is not None:\n",
        "                kpss = kpss[bindex, :]\n",
        "        return det, kpss\n",
        "\n",
        "    def nms(self, dets):\n",
        "        thresh = self.nms_thresh\n",
        "        x1 = dets[:, 0]\n",
        "        y1 = dets[:, 1]\n",
        "        x2 = dets[:, 2]\n",
        "        y2 = dets[:, 3]\n",
        "        scores = dets[:, 4]\n",
        "\n",
        "        areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "        order = scores.argsort()[::-1]\n",
        "\n",
        "        keep = []\n",
        "        while order.size > 0:\n",
        "            i = order[0]\n",
        "            keep.append(i)\n",
        "            xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "            yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "            xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "            yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "\n",
        "            w = np.maximum(0.0, xx2 - xx1 + 1)\n",
        "            h = np.maximum(0.0, yy2 - yy1 + 1)\n",
        "            inter = w * h\n",
        "            ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
        "\n",
        "            inds = np.where(ovr <= thresh)[0]\n",
        "            order = order[inds + 1]\n",
        "\n",
        "        return keep\n",
        "\n",
        "def get_retinaface(name, download=False, root='~/.insightface/models', **kwargs):\n",
        "    if not download:\n",
        "        assert os.path.exists(name)\n",
        "        return RetinaFace(name)\n",
        "    else:\n",
        "        from .model_store import get_model_file\n",
        "        _file = get_model_file(\"retinaface_%s\" % name, root=root)\n",
        "        return retinaface(_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pUZSIRLkK4iH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm as l2norm\n",
        "#from easydict import EasyDict\n",
        "\n",
        "class Face(dict):\n",
        "\n",
        "    def __init__(self, d=None, **kwargs):\n",
        "        if d is None:\n",
        "            d = {}\n",
        "        if kwargs:\n",
        "            d.update(**kwargs)\n",
        "        for k, v in d.items():\n",
        "            setattr(self, k, v)\n",
        "        # Class attributes\n",
        "        #for k in self.__class__.__dict__.keys():\n",
        "        #    if not (k.startswith('__') and k.endswith('__')) and not k in ('update', 'pop'):\n",
        "        #        setattr(self, k, getattr(self, k))\n",
        "\n",
        "    def __setattr__(self, name, value):\n",
        "        if isinstance(value, (list, tuple)):\n",
        "            value = [self.__class__(x)\n",
        "                    if isinstance(x, dict) else x for x in value]\n",
        "        elif isinstance(value, dict) and not isinstance(value, self.__class__):\n",
        "            value = self.__class__(value)\n",
        "        super(Face, self).__setattr__(name, value)\n",
        "        super(Face, self).__setitem__(name, value)\n",
        "\n",
        "    __setitem__ = __setattr__\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        return None\n",
        "\n",
        "    @property\n",
        "    def embedding_norm(self):\n",
        "        if self.embedding is None:\n",
        "            return None\n",
        "        return l2norm(self.embedding)\n",
        "\n",
        "    @property\n",
        "    def normed_embedding(self):\n",
        "        if self.embedding is None:\n",
        "            return None\n",
        "        return self.embedding / self.embedding_norm\n",
        "\n",
        "    @property\n",
        "    def sex(self):\n",
        "        if self.gender is None:\n",
        "            return None\n",
        "        return 'M' if self.gender==1 else 'F'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "TeWm1H1sLACT",
        "outputId": "c2fbf5f9-d84b-49f8-fb2e-20f9570adfa2"
      },
      "outputs": [
        {
          "ename": "SSLCertVerificationError",
          "evalue": "[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:1010)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/geventhttpclient/connectionpool.py\u001b[0m in \u001b[0;36mget_socket\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gevent/_gevent_cqueue.cpython-312-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mgevent._gevent_cqueue.SimpleQueue.get\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gevent/_gevent_cqueue.cpython-312-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mgevent._gevent_cqueue.SimpleQueue.get\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gevent/_gevent_cqueue.cpython-312-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mgevent._gevent_cqueue.SimpleQueue._SimpleQueue__get_or_peek\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mEmpty\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2738253850.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Run detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkpss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bounding boxes:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Keypoints:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkpss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4022855066.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(self, img, input_size, max_num, metric)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mdet_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnew_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mnew_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresized_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mscores_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkpss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdet_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4022855066.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, threshold)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriton_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mnet_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tritonclient/http/_client.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, model_name, inputs, model_version, outputs, request_id, sequence_id, sequence_start, sequence_end, priority, timeout, headers, query_params, request_compression_algorithm, response_compression_algorithm, parameters)\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0mrequest_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"v2/models/{}/infer\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m         response = self._post(\n\u001b[0m\u001b[1;32m   1477\u001b[0m             \u001b[0mrequest_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m             \u001b[0mrequest_body\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_body\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tritonclient/http/_client.py\u001b[0m in \u001b[0;36m_post\u001b[0;34m(self, request_uri, request_body, headers, query_params)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             response = self._client_stub.post(\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mrequest_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/geventhttpclient/client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, request_uri, body, headers)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETHOD_POST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/geventhttpclient/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, request_uri, body, headers)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0m_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/geventhttpclient/connectionpool.py\u001b[0m in \u001b[0;36mget_socket\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_semaphore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/geventhttpclient/connectionpool.py\u001b[0m in \u001b[0;36m_create_socket\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfirst_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mfirst_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cannot resolve {self._host}:{self._port}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/geventhttpclient/connectionpool.py\u001b[0m in \u001b[0;36m_create_socket\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msock_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/geventhttpclient/connectionpool.py\u001b[0m in \u001b[0;36m_connect_socket\u001b[0;34m(self, sock, address)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mserver_hostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssl_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"server_hostname\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_host\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;31m# SSLSocket class handles server_hostname encoding before it calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# ctx._wrap_socket()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         return self.sslsocket_class._create(\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mserver_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_side\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1039\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:1010)"
          ]
        }
      ],
      "source": [
        "# ---- RetinaFace test (Triton version) ----\n",
        "import cv2\n",
        "\n",
        "# Load the model using Triton backend\n",
        "# point to your Triton server URL + model name\n",
        "detector = RetinaFace(\n",
        "    triton_url=\"psstr3clf1dpd1p1e075ccpuqs.ingress.rtx4090.wyo.eg.akash.pub\",\n",
        "    model_name=\"buffalo_face_detection\"\n",
        ")\n",
        "\n",
        "# Prepare input size (same as before)\n",
        "detector.prepare(ctx_id=0, input_size=(640, 640))  # Triton CPU/GPU handled internally\n",
        "\n",
        "# Load an image\n",
        "img = cv2.imread(\"face.jpg\")\n",
        "\n",
        "\n",
        "\n",
        "# Run detection\n",
        "bboxes, kpss = detector.detect(img, input_size=(640, 640))\n",
        "print(\"Bounding boxes:\\n\", bboxes)\n",
        "print(\"Keypoints:\\n\", kpss)\n",
        "\n",
        "# Build Face objects (same as before)\n",
        "for i in range(bboxes.shape[0]):\n",
        "    bbox = bboxes[i, 0:4]\n",
        "    det_score = bboxes[i, 4]\n",
        "    kps = None\n",
        "    if kpss is not None:\n",
        "        kps = kpss[i]\n",
        "    face = Face(bbox=bbox, kps=kps, det_score=det_score)\n",
        "    print(face)\n",
        "\n",
        "# Optionally visualize\n",
        "for box in bboxes:\n",
        "    x1, y1, x2, y2, score = box.astype(int)\n",
        "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "    cv2.putText(img, f\"{score:.2f}\", (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)\n",
        "\n",
        "cv2.imwrite(\"retinaface_result.jpg\", img)\n",
        "print(\"Saved result as retinaface_result.jpg\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug4xhUUcJH1v",
        "outputId": "3b442748-9b99-40b2-ff05-6040ab8ce1b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\n",
            "Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.19.0\n"
          ]
        }
      ],
      "source": [
        "!pip install onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C5miWTJJniR",
        "outputId": "6fe0b9ce-2052-4931-81d4-ccfb092f3583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ONNX model is valid!\n",
            "\n",
            "Model Inputs:\n",
            "Name: images, Shape: ['None', 3, 'None', 'None'], Dtype: float32\n",
            "\n",
            "Model Outputs:\n",
            "Name: output0, Shape: ['None', 84, 'None'], Dtype: float32\n"
          ]
        }
      ],
      "source": [
        "import onnx\n",
        "from onnx import TensorProto\n",
        "\n",
        "# Helper function to map ONNX tensor types to numpy types\n",
        "def onnx_dtype_to_numpy(dtype):\n",
        "    mapping = {\n",
        "        TensorProto.FLOAT: \"float32\",\n",
        "        TensorProto.UINT8: \"uint8\",\n",
        "        TensorProto.INT8: \"int8\",\n",
        "        TensorProto.UINT16: \"uint16\",\n",
        "        TensorProto.INT16: \"int16\",\n",
        "        TensorProto.INT32: \"int32\",\n",
        "        TensorProto.INT64: \"int64\",\n",
        "        TensorProto.BOOL: \"bool\",\n",
        "        TensorProto.FLOAT16: \"float16\",\n",
        "        TensorProto.DOUBLE: \"float64\",\n",
        "    }\n",
        "    return mapping.get(dtype, \"unknown\")\n",
        "\n",
        "# Load the ONNX model\n",
        "model_path =\"/content/yolov8l.onnx\"\n",
        "model = onnx.load(model_path)\n",
        "onnx.checker.check_model(model)\n",
        "print(\"ONNX model is valid!\\n\")\n",
        "\n",
        "# Inspect inputs\n",
        "print(\"Model Inputs:\")\n",
        "for input in model.graph.input:\n",
        "    name = input.name\n",
        "    tensor_type = input.type.tensor_type\n",
        "    dtype = onnx_dtype_to_numpy(tensor_type.elem_type)\n",
        "    shape = [dim.dim_value if (dim.dim_value > 0) else 'None' for dim in tensor_type.shape.dim]\n",
        "    print(f\"Name: {name}, Shape: {shape}, Dtype: {dtype}\")\n",
        "\n",
        "# Inspect outputs\n",
        "print(\"\\nModel Outputs:\")\n",
        "for output in model.graph.output:\n",
        "    name = output.name\n",
        "    tensor_type = output.type.tensor_type\n",
        "    dtype = onnx_dtype_to_numpy(tensor_type.elem_type)\n",
        "    shape = [dim.dim_value if (dim.dim_value > 0) else 'None' for dim in tensor_type.shape.dim]\n",
        "    print(f\"Name: {name}, Shape: {shape}, Dtype: {dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x5ASc5lOnq4R",
        "outputId": "a701e4eb-16f2-4441-e88e-5c768f7a6b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.203)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.17)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
